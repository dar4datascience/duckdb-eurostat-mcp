# LLM Provider Configuration
# Choose which LLM provider to use: anthropic, openai, ollama, or azure
LLM_PROVIDER=anthropic

# Anthropic Configuration (default)
# Get your key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# OpenAI Configuration
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# Azure OpenAI Configuration
AZURE_OPENAI_API_KEY=your-azure-api-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT=your-deployment-name

# Ollama Configuration (local LLM)
# No API key needed - just ensure Ollama is running locally
# Default: http://localhost:11434
